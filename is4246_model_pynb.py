# -*- coding: utf-8 -*-
"""IS4246 Model.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o1gROBsgQNo9RIcVLTt0sRZYq551BZDc

Environment Setup
"""

"""1) Imports & Reproducibility"""

# --- imports & seeds ---
from io import StringIO
import sys
import os, random, json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import shap

from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                             precision_recall_curve, roc_curve, f1_score)

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""2) Data Loading & Preprocessing"""

# --- data loading & preprocessing ---
def preprocess_data(df, seed=SEED):
    dfp = df.copy()

    # encode categoricals
    categorical_cols = ['education', 'self_employed']
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        dfp[col] = le.fit_transform(dfp[col].astype(str))
        label_encoders[col] = le

    # target mapping
    status_map = {'Approved': 1, 'Rejected': 0}
    dfp['loan_status'] = dfp['loan_status'].map(status_map).astype(int)
    label_encoders['loan_status'] = status_map

    # base features
    base_cols = [
        'no_of_dependents','education','self_employed','income_annum',
        'loan_amount','loan_term','cibil_score','residential_assets_value',
        'commercial_assets_value','luxury_assets_value','bank_asset_value'
    ]

    X = dfp[base_cols].copy()
    y = dfp['loan_status'].copy()

    # derived feature
    asset_cols = ['residential_assets_value','commercial_assets_value',
                  'luxury_assets_value','bank_asset_value']
    X['has_debt'] = (X[asset_cols].lt(0).any(axis=1)).astype(int)

    feature_cols = base_cols + ['has_debt']

    # impute & split (70/15/15 stratified)
    X = X.fillna(X.median(numeric_only=True))
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.30, stratify=y, random_state=seed
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=seed
    )

    # scale using train only
    scaler = StandardScaler().fit(X_train)
    X_train = scaler.transform(X_train)
    X_val   = scaler.transform(X_val)
    X_test  = scaler.transform(X_test)

    return (X_train, y_train, X_val, y_val, X_test, y_test,
            scaler, label_encoders, feature_cols)

#Loading Data
DATA_PATH = os.path.join(os.path.dirname(__file__), "data", "loan_approval_dataset.csv")
df = pd.read_csv(DATA_PATH)

"""3) TensorDatasets & DataLoaders"""

# --- dataloaders ---
def to_tensor_ds(X, y):
    y_arr = y.values.reshape(-1, 1) if hasattr(y, "values") else np.array(y).reshape(-1, 1)
    return TensorDataset(torch.tensor(X, dtype=torch.float32),
                         torch.tensor(y_arr, dtype=torch.float32))

def make_loaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=64):
    train_ds = to_tensor_ds(X_train, y_train)
    val_ds   = to_tensor_ds(X_val,   y_val)
    test_ds  = to_tensor_ds(X_test,  y_test)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)
    return train_loader, val_loader, test_loader

"""4) Model Architecture"""

# --- model ---
class LoanApprovalModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.layer1 = nn.Linear(input_size, 512); self.bn1 = nn.BatchNorm1d(512)
        self.layer2 = nn.Linear(512, 256);        self.bn2 = nn.BatchNorm1d(256)
        self.layer3 = nn.Linear(256, 128);        self.bn3 = nn.BatchNorm1d(128)
        self.layer4 = nn.Linear(128,  64);        self.bn4 = nn.BatchNorm1d(64)
        self.layer5 = nn.Linear( 64,  32);        self.bn5 = nn.BatchNorm1d(32)
        self.dropout1 = nn.Dropout(0.4)
        self.dropout2 = nn.Dropout(0.3)
        self.dropout3 = nn.Dropout(0.2)
        self.output = nn.Linear(32, 1)  # logits

    def forward(self, x):
        x = F.relu(self.bn1(self.layer1(x))); x = self.dropout1(x)
        x = F.relu(self.bn2(self.layer2(x))); x = self.dropout2(x)
        x = F.relu(self.bn3(self.layer3(x))); x = self.dropout3(x)
        x = F.relu(self.bn4(self.layer4(x)))
        x = F.relu(self.bn5(self.layer5(x)))
        return self.output(x)  # raw logits

"""5) Loss function, Optimizer, Scheduler"""

# --- training config helpers ---
def make_training_objects(model, y_train):
    pos = float(y_train.sum())
    neg = float(len(y_train) - y_train.sum())
    pos_weight_value = (neg / max(pos, 1.0)) if pos > 0 else 1.0

    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value], device=DEVICE))
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
    return criterion, optimizer, scheduler

"""6) Model Training (with Early Stopping)"""

# --- training loop ---
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,
                epochs=10, early_stop_patience=20, ckpt_path='best_model.pth'):
    train_losses, val_losses, train_accs, val_accs, lrs = [], [], [], [], []
    best_val = float('inf'); patience = 0

    for epoch in range(epochs):
        # train
        model.train()
        tloss, correct, total = 0.0, 0, 0
        for x, y in train_loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            tloss += loss.item()
            preds = (torch.sigmoid(logits) > 0.5).float()
            correct += (preds == y).sum().item()
            total += y.size(0)

        train_loss = tloss / len(train_loader)
        train_acc = 100.0 * correct / total

        # val
        model.eval()
        vloss, vcorrect, vtotal = 0.0, 0, 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(DEVICE), y.to(DEVICE)
                logits = model(x)
                vloss += criterion(logits, y).item()
                preds = (torch.sigmoid(logits) > 0.5).float()
                vcorrect += (preds == y).sum().item()
                vtotal += y.size(0)

        val_loss = vloss / len(val_loader)
        val_acc = 100.0 * vcorrect / vtotal

        scheduler.step(val_loss)
        train_losses.append(train_loss); val_losses.append(val_loss)
        train_accs.append(train_acc);    val_accs.append(val_acc)
        lrs.append(optimizer.param_groups[0]['lr'])

        # early stopping
        if val_loss < best_val:
            best_val = val_loss; patience = 0
            torch.save(model.state_dict(), ckpt_path)
        else:
            patience += 1
            if patience >= early_stop_patience:
                break

    model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))
    return train_losses, val_losses, train_accs, val_accs, lrs

"""7) Threshold Tuning (by F1 on Validation)"""

# --- threshold tuning ---
def tune_threshold(model, loader):
    model.eval()
    probs_all, y_all = [], []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            probs = torch.sigmoid(model(x)).cpu().numpy().ravel()
            probs_all.append(probs); y_all.append(y.cpu().numpy().ravel())
    probs = np.concatenate(probs_all); y_true = np.concatenate(y_all)

    ths = np.linspace(0.05, 0.95, 19)
    f1s = []
    for t in ths:
        f1s.append(f1_score(y_true, (probs >= t).astype(int)))
    best_idx = int(np.argmax(f1s))
    return float(ths[best_idx]), float(f1s[best_idx])

"""8) Model Evaluation"""

# --- evaluation ---
def evaluate(model, loader, criterion, threshold=0.5, label_names=('Rejected','Approved')):
    model.eval()
    tloss, correct, total = 0.0, 0, 0
    probs_all, y_all = [], []

    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            tloss += criterion(logits, y).item()
            probs = torch.sigmoid(logits).cpu().numpy().ravel()
            probs_all.append(probs); y_all.append(y.cpu().numpy().ravel())
            preds = (probs >= threshold).astype(np.float32)
            correct += (preds == y.cpu().numpy().ravel()).sum()
            total += len(preds)

    loss = tloss / len(loader)
    probs = np.concatenate(probs_all); y_true = np.concatenate(y_all)
    acc = 100.0 * correct / total
    auc = roc_auc_score(y_true, probs)

    preds_bin = (probs >= threshold).astype(int)
    print(f"Loss: {loss:.4f} | Acc: {acc:.2f}% | AUC: {auc:.4f}\n")
    print("Classification Report:")
    print(classification_report(y_true, preds_bin, target_names=list(label_names)))

    cm = confusion_matrix(y_true, preds_bin)
    plt.figure(figsize=(5.5,4.5)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=list(label_names), yticklabels=list(label_names))
    plt.title('Confusion Matrix'); plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout(); plt.show()

    fpr, tpr, _ = roc_curve(y_true, probs)
    plt.figure(figsize=(5.5,4.5)); plt.plot(fpr, tpr, label=f'AUC={auc:.3f}'); plt.plot([0,1],[0,1],'--')
    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend(); plt.tight_layout(); plt.show()

    prec, rec, _ = precision_recall_curve(y_true, probs)
    plt.figure(figsize=(5.5,4.5)); plt.plot(rec, prec)
    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precisionâ€“Recall Curve'); plt.tight_layout(); plt.show()

    return acc, auc, probs, y_true

"""9) Permutation Feature Importance"""

# --- permutation feature importance ---
def permutation_importance(model, feature_cols, loader, top_k=10):
    model.eval()
    with torch.no_grad():
        batch_x, _ = next(iter(loader))
        batch_x = batch_x.to(DEVICE)
        base = torch.sigmoid(model(batch_x)).mean().item()

    n = batch_x.size(0)
    imps = {}
    for i, fname in enumerate(feature_cols):
        with torch.no_grad():
            perm = torch.randperm(n, device=DEVICE)
            pert = batch_x.clone()
            pert[:, i] = pert[perm, i]
            imp = base - torch.sigmoid(model(pert)).mean().item()
            imps[fname] = imp

    ranked = sorted(imps.items(), key=lambda x: abs(x[1]), reverse=True)
    feats, vals = zip(*ranked[:top_k])
    plt.figure(figsize=(7,4.5)); plt.barh(feats, vals); plt.xlabel('Î” mean prob'); plt.title('Top Feature Importance')
    plt.tight_layout(); plt.show()
    return ranked

"""10) Execution"""

# --- main run  ---
# 1) preprocess
X_train, y_train, X_val, y_val, X_test, y_test, scaler, encoders, feature_cols = preprocess_data(df)

# 2) loaders
train_loader, val_loader, test_loader = make_loaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=64)

# 3) model & training objects
model = LoanApprovalModel(input_size=len(feature_cols)).to(DEVICE)
criterion, optimizer, scheduler = make_training_objects(model, y_train)

# 4) train
train_losses, val_losses, train_accs, val_accs, lrs = train_model(
    model, train_loader, val_loader, criterion, optimizer, scheduler,
    epochs=10, early_stop_patience=20, ckpt_path='best_loan_model.pth'
)

# 5) tune threshold on validation
best_thr, best_f1 = tune_threshold(model, val_loader)
print(f"Best threshold (val F1): {best_thr:.3f} | F1={best_f1:.4f}")

# 6) evaluate on test
acc, auc, probs, y_true = evaluate(model, test_loader, criterion, threshold=best_thr)

# 7) feature importance
ranked_imps = permutation_importance(model, feature_cols, test_loader, top_k=10)

"""11. Equalised Odds

"""

def equalised_odds_by_group(y_true, y_pred, group_values, group_name="Group"):
    """
    Compute Equalised Odds metrics (TPR/FPR per group) and their differences.

    Parameters:
        y_true (array-like): True binary labels (0/1)
        y_pred (array-like): Predicted binary labels (0/1)
        group_values (array-like): Sensitive attribute values (e.g. education levels)
        group_name (str): Name of the group for reporting

    Returns:
        results_df (pd.DataFrame): TPR/FPR for each group
        eo_diff (dict): Differences in TPR/FPR across groups
    """
    df = pd.DataFrame({
        'y_true': y_true,
        'y_pred': y_pred,
        'group': group_values
    })

    results = []
    for g, sub in df.groupby('group'):
        tn, fp, fn, tp = confusion_matrix(sub['y_true'], sub['y_pred']).ravel()
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # recall for positive class
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # false positive rate
        results.append({'Group': g, 'TPR': tpr, 'FPR': fpr})

    results_df = pd.DataFrame(results)
    tpr_diff = results_df['TPR'].max() - results_df['TPR'].min()
    fpr_diff = results_df['FPR'].max() - results_df['FPR'].min()

    print(f"\n=== Equalised Odds for {group_name} ===")
    print(results_df.to_string(index=False))
    print(f"\nTPR Difference: {tpr_diff:.4f}")
    print(f"FPR Difference: {fpr_diff:.4f}")

    eo_diff = {'TPR_diff': tpr_diff, 'FPR_diff': fpr_diff}
    return results_df, eo_diff

"""12) EOD Results"""

# true labels and predictions from your test set
y_pred = (probs >= best_thr).astype(int)

# subgroup columns (assuming from your original dataframe or label encoder)
education_groups = df.loc[y_test.index, 'education']
employment_groups = df.loc[y_test.index, 'self_employed']
dependents_groups = df.loc[y_test.index, 'no_of_dependents']

# compute equalised odds per attribute
eq_edu, edu_diff = equalised_odds_by_group(y_true, y_pred, education_groups, group_name="Education")
eq_emp, emp_diff = equalised_odds_by_group(y_true, y_pred, employment_groups, group_name="Employment Type")
eq_dep, dep_diff = equalised_odds_by_group(y_true, y_pred, dependents_groups, group_name="Number of Dependents")

"""13) SHAP Evaluation"""

# --- Simple SHAP analysis for model interpretability ---
def simple_shap_analysis(model, feature_cols, X_test, y_test, probs, threshold=0.5):
    """
    Simple SHAP analysis to show feature importance and customer decisions
    """
    print("=== SHAP FEATURE IMPORTANCE ===")

    # Use a sample of test data for SHAP (faster)
    X_sample = X_test[:100]

    # Create SHAP explainer
    def model_predict(x):
        model.eval()
        with torch.no_grad():
            x_tensor = torch.tensor(x, dtype=torch.float32).to(DEVICE)
            return torch.sigmoid(model(x_tensor)).cpu().numpy()

    # Use training data as background
    explainer = shap.Explainer(model_predict, X_train[:50])
    shap_values = explainer(X_sample)

    # Show global feature importance
    mean_abs_shap = np.mean(np.abs(shap_values.values), axis=0)
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': mean_abs_shap
    }).sort_values('importance', ascending=False)

    print("\nðŸ“Š TOP 10 FEATURES BY IMPORTANCE:")
    print(feature_importance.head(10).to_string(index=False))

    return shap_values, feature_importance

def show_customer_decisions(feature_cols, X_test, y_test, probs, scaler, threshold=0.5, num_customers=10):
    """
    Show loan decisions for test customers with detailed background and explanations
    """
    print(f"\n{'='*80}")
    print("DETAILED LOAN DECISIONS ANALYSIS FOR TEST CUSTOMERS")
    print(f"{'='*80}")

    # Get predictions
    y_pred = (probs >= threshold).astype(int)

    # Select random customers from test set
    indices = np.random.choice(len(X_test), min(num_customers, len(X_test)), replace=False)

    for i, idx in enumerate(indices):
        customer_features = X_test[idx]
        true_status = "Approved" if y_test.iloc[idx] == 1 else "Rejected"
        pred_prob = probs[idx]
        pred_status = "APPROVED" if y_pred[idx] == 1 else "REJECTED"
        confidence = "HIGH" if abs(pred_prob - 0.5) > 0.3 else "MEDIUM"

        # Convert back to original scale for interpretation
        customer_original = scaler.inverse_transform(customer_features.reshape(1, -1))[0]
        feature_dict = dict(zip(feature_cols, customer_original))

        print(f"\nðŸŽ¯ CUSTOMER {i+1} PROFILE")
        print(f"â”" * 50)
        print(f"ðŸ“‹ DECISION SUMMARY:")
        print(f"   â€¢ Final Decision: {pred_status}")
        print(f"   â€¢ Confidence Level: {confidence}")
        print(f"   â€¢ Approval Probability: {pred_prob:.1%}")
        print(f"   â€¢ Actual Outcome: {true_status}")
        print(f"   â€¢ Risk Level: {'LOW' if pred_status == 'APPROVED' else 'HIGH'}")

        print(f"\nðŸ‘¤ CUSTOMER BACKGROUND:")
        print(f"   â€¢ Education Level: {get_education_level(feature_dict)}")
        print(f"   â€¢ Employment Type: {get_employment_type(feature_dict)}")
        print(f"   â€¢ Dependents: {int(feature_dict.get('no_of_dependents', 0))} people")
        print(f"   â€¢ Annual Income: ${feature_dict.get('income_annum', 0):,.0f}")

        print(f"\nðŸ’° FINANCIAL PROFILE:")
        print(f"   â€¢ Loan Amount Requested: ${feature_dict.get('loan_amount', 0):,.0f}")
        print(f"   â€¢ Loan Term: {int(feature_dict.get('loan_term', 0))} years")
        print(f"   â€¢ Credit Score (CIBIL): {int(feature_dict.get('cibil_score', 0))}")
        print(f"   â€¢ Total Assets: ${feature_dict.get('residential_assets_value', 0) + feature_dict.get('commercial_assets_value', 0) + feature_dict.get('luxury_assets_value', 0) + feature_dict.get('bank_asset_value', 0):,.0f}")

        # Calculate key ratios (CORRECTED - loan term in YEARS)
        income = max(feature_dict.get('income_annum', 1), 1)
        loan_amount = max(feature_dict.get('loan_amount', 1), 1)
        loan_term_years = max(feature_dict.get('loan_term', 1), 1)
        loan_term_months = loan_term_years * 12

        # Debt-to-Income Ratio (annual)
        debt_to_income = loan_amount / income

        # Monthly payment estimate (with simple interest assumption)
        # Assuming 10% annual interest for calculation purposes
        annual_interest_rate = 0.10
        monthly_interest = annual_interest_rate / 12
        if loan_term_months > 0:
            monthly_payment = loan_amount * (monthly_interest * (1 + monthly_interest)**loan_term_months) / ((1 + monthly_interest)**loan_term_months - 1)
        else:
            monthly_payment = 0

        monthly_income = income / 12
        payment_to_income = monthly_payment / monthly_income if monthly_income > 0 else 0

        print(f"\nðŸ“Š FINANCIAL RATIOS:")
        print(f"   â€¢ Debt-to-Income Ratio: {debt_to_income:.2f} (loan amount / annual income)")
        print(f"   â€¢ Estimated Monthly Payment: ${monthly_payment:,.0f}")
        print(f"   â€¢ Payment-to-Income Ratio: {payment_to_income:.1%} of monthly income")
        print(f"   â€¢ Loan Term: {loan_term_years} years ({loan_term_months} months)")

        # Decision factors
        if pred_status == "REJECTED":
            print(f"\nðŸ”´ KEY REJECTION FACTORS:")
            risky_factors = identify_risk_factors(feature_dict)
            for j, (factor, severity, details) in enumerate(risky_factors[:3]):
                print(f"   {j+1}. [{severity}] {factor}")
                print(f"      {details}")
        else:
            print(f"\nðŸŸ¢ KEY APPROVAL STRENGTHS:")
            strong_factors = identify_strength_factors(feature_dict)
            for j, (factor, strength, details) in enumerate(strong_factors[:3]):
                print(f"   {j+1}. [{strength}] {factor}")
                print(f"      {details}")

        # Recommendation
        print(f"\nðŸ’¡ RECOMMENDATION:")
        if pred_status == "REJECTED":
            print(f"   Consider improving: {get_improvement_suggestions(risky_factors)}")
        else:
            print(f"   Strong candidate - meets all key approval criteria")

        print(f"â”" * 50)

def get_education_level(feature_dict):
    """Convert education value to readable format"""
    edu_value = feature_dict.get('education', 0)
    # Assuming: 0 = Not Graduate, 1 = Graduate
    return "Graduate" if edu_value >= 0.5 else "Not Graduate"

def get_employment_type(feature_dict):
    """Convert employment value to readable format"""
    emp_value = feature_dict.get('self_employed', 0)
    # Assuming: 0 = Salaried, 1 = Self-Employed
    return "Self-Employed" if emp_value >= 0.5 else "Salaried"

def identify_risk_factors(feature_dict):
    """Identify and rank risk factors for rejection"""
    risk_factors = []

    # CIBIL Score risk
    cibil = feature_dict.get('cibil_score', 0)
    if cibil < 600:
        risk_factors.append((
            "Poor Credit History",
            "CRITICAL",
            f"CIBIL score of {cibil:.0f} is below minimum requirement (600+)"
        ))
    elif cibil < 700:
        risk_factors.append((
            "Moderate Credit Score",
            "HIGH",
            f"CIBIL score of {cibil:.0f} indicates some credit risk"
        ))

    # Income to Loan ratio
    income = feature_dict.get('income_annum', 1)
    loan_amount = feature_dict.get('loan_amount', 1)
    if loan_amount > 0:
        ratio = income / loan_amount
        if ratio < 0.3:
            risk_factors.append((
                "High Debt Burden",
                "CRITICAL",
                f"Annual income is only {ratio:.1%} of loan amount"
            ))
        elif ratio < 0.5:
            risk_factors.append((
                "Moderate Debt Burden",
                "HIGH",
                f"Annual income is {ratio:.1%} of loan amount"
            ))

    # Calculate actual monthly payment burden
    income = max(feature_dict.get('income_annum', 1), 1)
    loan_amount = max(feature_dict.get('loan_amount', 1), 1)
    loan_term_years = max(feature_dict.get('loan_term', 1), 1)
    loan_term_months = loan_term_years * 12

    # Monthly payment estimate
    annual_interest_rate = 0.10
    monthly_interest = annual_interest_rate / 12
    if loan_term_months > 0:
        monthly_payment = loan_amount * (monthly_interest * (1 + monthly_interest)**loan_term_months) / ((1 + monthly_interest)**loan_term_months - 1)
    else:
        monthly_payment = 0

    monthly_income = income / 12
    payment_to_income = monthly_payment / monthly_income if monthly_income > 0 else 0

    # Payment burden risk
    if payment_to_income > 0.5:
        risk_factors.append((
            "Very High Payment Burden",
            "CRITICAL",
            f"Monthly payment is {payment_to_income:.1%} of monthly income (>50% is risky)"
        ))
    elif payment_to_income > 0.4:
        risk_factors.append((
            "High Payment Burden",
            "HIGH",
            f"Monthly payment is {payment_to_income:.1%} of monthly income"
        ))
    elif payment_to_income > 0.3:
        risk_factors.append((
            "Moderate Payment Burden",
            "MEDIUM",
            f"Monthly payment is {payment_to_income:.1%} of monthly income"
        ))

    # Asset coverage
    total_assets = (feature_dict.get('residential_assets_value', 0) +
                   feature_dict.get('commercial_assets_value', 0) +
                   feature_dict.get('luxury_assets_value', 0) +
                   feature_dict.get('bank_asset_value', 0))

    if total_assets < loan_amount * 0.5:
        risk_factors.append((
            "Insufficient Collateral",
            "MEDIUM",
            f"Assets cover only {total_assets/loan_amount:.1%} of loan amount"
        ))

    # Many dependents
    dependents = feature_dict.get('no_of_dependents', 0)
    if dependents >= 4:
        risk_factors.append((
            "High Number of Dependents",
            "MEDIUM",
            f"{dependents} dependents may strain financial capacity"
        ))

    return sorted(risk_factors, key=lambda x: x[1] != "CRITICAL")

def identify_strength_factors(feature_dict):
    """Identify and rank strength factors for approval"""
    strength_factors = []

    # CIBIL Score strength
    cibil = feature_dict.get('cibil_score', 0)
    if cibil >= 750:
        strength_factors.append((
            "Excellent Credit History",
            "VERY STRONG",
            f"CIBIL score of {cibil:.0f} indicates excellent creditworthiness"
        ))
    elif cibil >= 700:
        strength_factors.append((
            "Good Credit History",
            "STRONG",
            f"CIBIL score of {cibil:.0f} meets all credit requirements"
        ))

    # Income strength
    income = feature_dict.get('income_annum', 1)
    loan_amount = max(feature_dict.get('loan_amount', 1), 1)
    if loan_amount > 0:
        ratio = income / loan_amount
        if ratio >= 0.7:
            strength_factors.append((
                "Strong Income Coverage",
                "VERY STRONG",
                f"Annual income is {ratio:.1%} of loan amount"
            ))
        elif ratio >= 0.5:
            strength_factors.append((
                "Good Income Coverage",
                "STRONG",
                f"Annual income is {ratio:.1%} of loan amount"
            ))

    # Calculate actual monthly payment burden for strengths
    loan_term_years = max(feature_dict.get('loan_term', 1), 1)
    loan_term_months = loan_term_years * 12

    annual_interest_rate = 0.10
    monthly_interest = annual_interest_rate / 12
    if loan_term_months > 0:
        monthly_payment = loan_amount * (monthly_interest * (1 + monthly_interest)**loan_term_months) / ((1 + monthly_interest)**loan_term_months - 1)
    else:
        monthly_payment = 0

    monthly_income = income / 12
    payment_to_income = monthly_payment / monthly_income if monthly_income > 0 else 0

    # Low payment burden strength
    if payment_to_income < 0.25:
        strength_factors.append((
            "Low Payment Burden",
            "STRONG",
            f"Monthly payment is only {payment_to_income:.1%} of monthly income"
        ))
    elif payment_to_income < 0.35:
        strength_factors.append((
            "Reasonable Payment Burden",
            "MEDIUM",
            f"Monthly payment is {payment_to_income:.1%} of monthly income"
        ))

    # Asset strength
    total_assets = (feature_dict.get('residential_assets_value', 0) +
                   feature_dict.get('commercial_assets_value', 0) +
                   feature_dict.get('luxury_assets_value', 0) +
                   feature_dict.get('bank_asset_value', 0))

    if total_assets >= loan_amount * 2:
        strength_factors.append((
            "Very Strong Asset Backing",
            "VERY STRONG",
            f"Assets are {total_assets/loan_amount:.1f}x the loan amount"
        ))
    elif total_assets >= loan_amount:
        strength_factors.append((
            "Strong Asset Backing",
            "STRONG",
            f"Assets fully cover the loan amount"
        ))

    return sorted(strength_factors, key=lambda x: x[1] != "VERY STRONG", reverse=True)

def get_improvement_suggestions(risky_factors):
    """Provide improvement suggestions based on risk factors"""
    suggestions = []

    for factor, severity, details in risky_factors[:2]:
        if "Credit" in factor:
            suggestions.append("improve credit score by clearing outstanding debts")
        elif "Income" in factor or "Debt" in factor:
            suggestions.append("reduce loan amount or increase income")
        elif "Asset" in factor:
            suggestions.append("provide additional collateral")
        elif "Payment" in factor:
            suggestions.append("extend loan term to reduce monthly payments")

    return ", ".join(suggestions) if suggestions else "review financial profile"

# Execute detailed SHAP analysis
print("ðŸš€ Performing Detailed SHAP Analysis...")

# Get feature importance
shap_values, feature_importance = simple_shap_analysis(
    model, feature_cols, X_test, y_test, probs, best_thr
)

# Show detailed customer decisions with background
show_customer_decisions(
    feature_cols, X_test, y_test, probs, scaler, best_thr, num_customers=8
)

print(f"\nâœ… Detailed SHAP analysis complete!")